{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy scikit-learn rank-bm25 sentence-transformers umap-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# 5ï¸âƒ£ Ã‰valuation du moteur de recherche\n",
    "# ===============================================\n",
    "import numpy as np\n",
    "\n",
    "# ğŸ”¹ EntrÃ©es : \n",
    "# embedding_results = {query_id: [top-k IDs de documents rÃ©cupÃ©rÃ©s]}\n",
    "# gts_dict = {query_id: [IDs de documents pertinents]}  # vÃ©ritÃ© terrain\n",
    "\n",
    "# Exemple :\n",
    "# gts_dict = json.load(open(\"dataset/gts.json\"))\n",
    "\n",
    "k = 10  # Top-k documents Ã  Ã©valuer\n",
    "\n",
    "# ===============================================\n",
    "# ğŸ”¹ Ã‰tape 1 â€” Precision@k\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    hits = sum([1 for doc in retrieved_k if doc in relevant_set])\n",
    "    return hits / k\n",
    "\n",
    "# ===============================================\n",
    "# ğŸ”¹ Ã‰tape 2 â€” Recall@k\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    hits = sum([1 for doc in retrieved_k if doc in relevant_set])\n",
    "    return hits / len(relevant_set) if relevant_set else 0\n",
    "\n",
    "# ===============================================\n",
    "# ğŸ”¹ Ã‰tape 3 â€” Average Precision pour une requÃªte\n",
    "def average_precision(retrieved, relevant, k):\n",
    "    relevant_set = set(relevant)\n",
    "    hits = 0\n",
    "    sum_precisions = 0\n",
    "    for i, doc_id in enumerate(retrieved[:k]):\n",
    "        if doc_id in relevant_set:\n",
    "            hits += 1\n",
    "            sum_precisions += hits / (i + 1)\n",
    "    return sum_precisions / min(len(relevant_set), k) if relevant_set else 0\n",
    "\n",
    "# ===============================================\n",
    "# ğŸ”¹ Ã‰tape 4 â€” Ã‰valuer toutes les requÃªtes\n",
    "precisions, recalls, ap_scores = [], [], []\n",
    "\n",
    "for query_id, retrieved_docs in embedding_results.items():\n",
    "    relevant_docs = gts_dict.get(query_id, [])\n",
    "    \n",
    "    p = precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "    r = recall_at_k(retrieved_docs, relevant_docs, k)\n",
    "    ap = average_precision(retrieved_docs, relevant_docs, k)\n",
    "    \n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "    ap_scores.append(ap)\n",
    "\n",
    "# ===============================================\n",
    "# ğŸ”¹ Ã‰tape 5 â€” Calculer les mÃ©triques moyennes\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "mean_ap = np.mean(ap_scores)  # MAP\n",
    "\n",
    "print(f\"Precision@{k} : {mean_precision:.4f}\")\n",
    "print(f\"Recall@{k} : {mean_recall:.4f}\")\n",
    "print(f\"MAP@{k} : {mean_ap:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
